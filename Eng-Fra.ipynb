{
 "cells": [
  {
   "cell_type": "raw",
   "id": "8faaa0fd",
   "metadata": {},
   "source": [
    "##https://stackabuse.com/python-for-nlp-neural-machine-translation-with-seq2seq-in-keras/\n",
    "\n",
    "## https://machinelearningmastery.com/define-encoder-decoder-sequence-sequence-model-neural-machine-translation-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41f550d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, GRU, Dense, Embedding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "#import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2e2489d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the value for following parameter\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 2\n",
    "LSTM_NODES =256\n",
    "NUM_SENTENCES = 10000\n",
    "MAX_SENTENCE_LENGTH = 50\n",
    "MAX_NUM_WORDS = 20000\n",
    "EMBEDDING_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b0faa0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num samples input: 10000\n",
      "num samples output: 10000\n",
      "num samples output input: 10000\n"
     ]
    }
   ],
   "source": [
    "##Load the data set\n",
    "input_sentences = []\n",
    "output_sentences = []\n",
    "output_sentences_inputs = []\n",
    "\n",
    "count = 0\n",
    "for line in open(r'E:\\Pycharm_Projects\\tf_prac\\Encoder,Decoder,Seq2Seq\\eng-fra\\fra.txt', encoding=\"utf-8\"):\n",
    "    count += 1\n",
    "\n",
    "    if count > NUM_SENTENCES:\n",
    "        break\n",
    "\n",
    "    if '\\t' not in line:\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    line.rstrip().split('\\t')\n",
    "    input_sentence  = line.split('\\t')[0]\n",
    "    output = line.split('\\t')[1]\n",
    "\n",
    "    output_sentence = output + ' <eos>'\n",
    "    output_sentence_input = '<sos> ' + output\n",
    "\n",
    "    input_sentences.append(input_sentence)\n",
    "    output_sentences.append(output_sentence)\n",
    "    output_sentences_inputs.append(output_sentence_input)\n",
    "\n",
    "print(\"num samples input:\", len(input_sentences))\n",
    "print(\"num samples output:\", len(output_sentences))\n",
    "print(\"num samples output input:\", len(output_sentences_inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b6e91ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Be fair.\n",
      "Sois sincère. <eos>\n",
      "<sos> Sois sincère.\n"
     ]
    }
   ],
   "source": [
    "#now randomly print sentence\n",
    "print(input_sentences[172])\n",
    "print(output_sentences[172])\n",
    "print(output_sentences_inputs[172])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "be0abe4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique words in the input: 2014\n",
      "Length of longest sentence in input: 4\n"
     ]
    }
   ],
   "source": [
    "## Tokenization\n",
    "\n",
    "input_tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "\n",
    "input_tokenizer.fit_on_texts(input_sentences)\n",
    "input_integer_seq = input_tokenizer.texts_to_sequences(input_sentences) #token to seq of interger\n",
    "\n",
    "\n",
    "\n",
    "word2idx_inputs = input_tokenizer.word_index  #unique integer assigned to each word\n",
    "print('Total unique words in the input: %s' % len(word2idx_inputs))\n",
    "\n",
    "max_input_len = max(len(sen) for sen in input_integer_seq)\n",
    "print(\"Length of longest sentence in input: %g\" % max_input_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "adc07779",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('i', 1), ('tom', 2), ('it', 3), ('you', 4), (\"i'm\", 5), ('is', 6), ('a', 7), ('me', 8), ('we', 9), ('go', 10), (\"you're\", 11), ('are', 12), ('was', 13), (\"it's\", 14), ('be', 15), ('he', 16), ('this', 17), ('get', 18), (\"we're\", 19), ('up', 20), ('can', 21), ('that', 22), (\"don't\", 23), ('do', 24), ('take', 25), ('they', 26), (\"i'll\", 27), ('come', 28), ('to', 29), ('let', 30), (\"that's\", 31), ('have', 32), ('the', 33), ('keep', 34), ('here', 35), ('no', 36), ('stay', 37), ('not', 38), ('out', 39), ('in', 40), (\"they're\", 41), ('did', 42), ('who', 43), ('stop', 44), ('got', 45), ('him', 46), (\"tom's\", 47), ('need', 48), ('my', 49), ('she', 50), ('am', 51), ('us', 52), ('love', 53), ('lost', 54), ('like', 55), ('help', 56), ('what', 57), (\"he's\", 58), (\"let's\", 59), ('home', 60), ('on', 61), ('look', 62), ('all', 63), ('back', 64), ('see', 65), ('saw', 66), ('how', 67), ('try', 68), ('must', 69), ('one', 70), ('down', 71), ('please', 72), ('away', 73), ('feel', 74), ('busy', 75), ('want', 76), ('wait', 77), ('so', 78), ('now', 79), ('good', 80), (\"can't\", 81), ('hurt', 82), (\"we'll\", 83), ('felt', 84), ('know', 85), ('them', 86), ('just', 87), ('again', 88), ('leave', 89), ('safe', 90), ('run', 91), ('alone', 92), ('your', 93), ('ok', 94), ('had', 95), ('will', 96), ('too', 97), ('mine', 98), ('at', 99), ('there', 100), ('ask', 101), ('may', 102), ('work', 103), ('for', 104), ('won', 105), ('nice', 106), ('ready', 107), ('call', 108), ('eat', 109), ('sad', 110), ('well', 111), ('hate', 112), ('broke', 113), ('easy', 114), ('time', 115), ('win', 116), ('done', 117), ('went', 118), ('give', 119), ('made', 120), ('were', 121), ('still', 122), ('job', 123), ('over', 124), ('has', 125), ('happy', 126), ('ahead', 127), ('talk', 128), ('careful', 129), ('check', 130), ('drink', 131), ('sit', 132), ('quiet', 133), ('right', 134), ('bring', 135), ('left', 136), ('fun', 137), (\"i've\", 138), ('calm', 139), ('move', 140), (\"who's\", 141), ('walk', 142), ('early', 143), ('yours', 144), ('wrong', 145), ('care', 146), ('sick', 147), ('came', 148), ('with', 149), ('cool', 150), ('crazy', 151), ('beat', 152), ('free', 153), ('bed', 154), ('old', 155), ('tired', 156), ('her', 157), ('dog', 158), ('buy', 159), ('cold', 160), ('die', 161), ('going', 162), ('big', 163), ('nuts', 164), ('hold', 165), ('fat', 166), ('off', 167), ('lazy', 168), ('swim', 169), ('drunk', 170), ('read', 171), ('start', 172), ('mad', 173), ('more', 174), ('quit', 175), ('way', 176), ('drive', 177), ('late', 178), ('sure', 179), ('new', 180), ('say', 181), ('trust', 182), ('sing', 183), ('first', 184), ('bad', 185), (\"what's\", 186), ('found', 187), ('prepared', 188), ('could', 189), (\"didn't\", 190), ('make', 191), ('stand', 192), ('funny', 193), ('naive', 194), ('thanks', 195), ('fair', 196), ('open', 197), ('push', 198), ('forget', 199), ('shot', 200), (\"it'll\", 201), ('car', 202), ('hang', 203), ('grab', 204), ('hurry', 205), ('said', 206), ('clean', 207), ('lying', 208), ('lonely', 209), ('lot', 210), ('very', 211), ('relax', 212), ('sorry', 213), ('tried', 214), ('use', 215), ('pay', 216), ('fine', 217), ('rich', 218), ('terrific', 219), ('lucky', 220), ('hard', 221), ('tight', 222), ('turn', 223), ('loved', 224), ('break', 225), ('door', 226), ('shut', 227), ('tell', 228), ('wake', 229), ('weak', 230), ('hurts', 231), ('died', 232), ('watch', 233), ('awful', 234), ('hot', 235), ('cut', 236), ('outside', 237), ('liked', 238), ('never', 239), ('called', 240), ('stupid', 241), ('sleep', 242), ('gun', 243), ('looks', 244), ('fired', 245), ('find', 246), ('cute', 247), ('fast', 248), ('speak', 249), ('alive', 250), ('angry', 251), ('serious', 252), ('moving', 253), (\"won't\", 254), ('coming', 255), ('vote', 256), ('drop', 257), ('lie', 258), ('write', 259), ('seated', 260), ('inside', 261), ('weird', 262), ('mean', 263), ('cook', 264), ('tough', 265), ('true', 266), ('life', 267), ('famous', 268), ('hungry', 269), ('close', 270), ('caught', 271), ('great', 272), ('fell', 273), ('kill', 274), ('lock', 275), ('some', 276), ('cry', 277), ('dead', 278), ('pardon', 279), ('sleepy', 280), ('ignore', 281), ('kidding', 282), ('where', 283), ('why', 284), (\"you've\", 285), ('young', 286), ('loves', 287), ('nobody', 288), ('an', 289), (\"isn't\", 290), ('show', 291), ('touch', 292), ('soon', 293), ('naked', 294), ('loosen', 295), ('rest', 296), (\"she's\", 297), ('remember', 298), ('told', 299), ('trying', 300), (\"tom'll\", 301), ('anybody', 302), ('should', 303), ('does', 304), ('everyone', 305), ('nothing', 306), ('scared', 307), ('focused', 308), ('kind', 309), ('agree', 310), ('cried', 311), ('smoke', 312), ('ill', 313), ('shy', 314), ('rude', 315), ('tall', 316), ('works', 317), ('relaxed', 318), ('live', 319), ('armed', 320), ('dying', 321), ('of', 322), ('step', 323), ('knows', 324), ('hired', 325), ('guess', 326), ('water', 327), ('once', 328), ('pick', 329), ('these', 330), ('merciful', 331), ('yourself', 332), ('son', 333), ('beer', 334), ('bored', 335), ('fined', 336), ('mary', 337), ('french', 338), ('slowly', 339), ('took', 340), ('ate', 341), ('unbelievable', 342), ('smile', 343), ('hug', 344), ('slow', 345), ('join', 346), ('sign', 347), ('brave', 348), ('fix', 349), ('helped', 350), ('deaf', 351), ('ugly', 352), ('strong', 353), ('follow', 354), ('luck', 355), ('retired', 356), ('woke', 357), ('awake', 358), ('dizzy', 359), ('short', 360), ('stuck', 361), ('ours', 362), ('clever', 363), ('enjoy', 364), ('met', 365), ('losing', 366), ('normal', 367), ('sweet', 368), ('problem', 369), ('needs', 370), ('cash', 371), ('eyes', 372), ('needed', 373), ('saved', 374), ('reading', 375), ('fake', 376), ('memorize', 377), ('date', 378), ('send', 379), ('quickly', 380), ('kid', 381), ('cat', 382), ('silly', 383), ('play', 384), ('book', 385), ('cruel', 386), ('tie', 387), ('shoe', 388), ('crafty', 389), ('jump', 390), ('hello', 391), ('hit', 392), ('man', 393), ('brief', 394), ('talked', 395), ('his', 396), ('taste', 397), ('knew', 398), ('after', 399), ('catch', 400), ('dogs', 401), ('hope', 402), ('burned', 403), ('food', 404), ('seriously', 405), ('wonderful', 406), ('around', 407), ('tea', 408), ('joking', 409), ('skinny', 410), ('sneaky', 411), ('study', 412), ('gone', 413), ('bother', 414), ('age', 415), ('upset', 416), ('kids', 417), ('touched', 418), ('trapped', 419), ('unlucky', 420), ('crying', 421), ('joke', 422), ('best', 423), ('hand', 424), ('shame', 425), (\"they'll\", 426), ('tricked', 427), ('only', 428), ('hi', 429), ('fire', 430), ('lied', 431), ('paid', 432), ('listen', 433), ('awesome', 434), ('spoke', 435), ('looked', 436), ('smiled', 437), ('thin', 438), ('honest', 439), ('head', 440), ('gave', 441), (\"i'd\", 442), ('blind', 443), ('hers', 444), ('smell', 445), ('anyone', 446), ('wine', 447), ('forgive', 448), ('canceled', 449), ('last', 450), ('shoot', 451), ('better', 452), ('direct', 453), ('loaded', 454), ('shaken', 455), ('boy', 456), ('creative', 457), ('discreet', 458), ('punctual', 459), ('thorough', 460), ('seat', 461), ('dance', 462), ('fear', 463), ('both', 464), ('dancing', 465), ('driving', 466), ('working', 467), ('many', 468), ('wants', 469), ('might', 470), ('nearby', 471), ('yelling', 472), ('cops', 473), ('and', 474), ('room', 475), ('outrank', 476), ('wanted', 477), ('begin', 478), ('oh', 479), ('really', 480), ('goodbye', 481), ('welcome', 482), ('ran', 483), ('cover', 484), ('rested', 485), ('helps', 486), ('odd', 487), ('red', 488), ('fly', 489), ('excuse', 490), ('crashed', 491), ('obey', 492), ('dirty', 493), ('warm', 494), ('patient', 495), ('prudent', 496), ('duty', 497), ('smart', 498), ('worry', 499), ('hear', 500), ('promised', 501), ('sat', 502), ('liar', 503), ('sharp', 504), ('bit', 505), ('war', 506), ('deal', 507), ('fight', 508), ('finish', 509), ('started', 510), ('change', 511), ('worried', 512), ('money', 513), ('remain', 514), ('by', 515), ('lose', 516), ('miss', 517), ('act', 518), ('confident', 519), ('our', 520), ('resist', 521), ('school', 522), ('denied', 523), ('dumped', 524), ('likes', 525), ('admire', 526), ('key', 527), ('boss', 528), ('idea', 529), ('enough', 530), ('running', 531), ('singing', 532), ('exhausted', 533), ('terrible', 534), ('burn', 535), ('stood', 536), ('fit', 537), ('failed', 538), ('stayed', 539), ('bald', 540), ('save', 541), ('faster', 542), ('ski', 543), ('cheated', 544), ('fussy', 545), ('thank', 546), ('cares', 547), ('content', 548), ('face', 549), ('built', 550), ('chuckled', 551), ('twin', 552), ('greedy', 553), ('blue', 554), ('aside', 555), ('decide', 556), ('missed', 557), ('friendly', 558), ('ruthless', 559), ('asleep', 560), ('curious', 561), ('drank', 562), ('messed', 563), ('eggs', 564), ('moved', 565), ('baffled', 566), ('excited', 567), ('married', 568), ('fool', 569), ('starved', 570), ('stunned', 571), ('winning', 572), ('broken', 573), ('list', 574), ('think', 575), ('twins', 576), ('objective', 577), ('upstairs', 578), ('girls', 579), ('myself', 580), ('proof', 581), ('music', 582), ('addicted', 583), ('fighting', 584), ('finished', 585), ('innocent', 586), (\"wasn't\", 587), ('ended', 588), ('while', 589), ('hates', 590), ('afraid', 591), ('guilty', 592), ('stoned', 593), ('favor', 594), ('gear', 595), ('fed', 596), ('plan', 597), ('skiing', 598), ('rescued', 599), ('vulgar', 600), ('friends', 601), ('seem', 602), ('bright', 603), ('grumpy', 604), ('hide', 605), ('attack', 606), ('pass', 607), ('runs', 608), ('wet', 609), ('kiss', 610), ('pull', 611), ('aim', 612), ('obeyed', 613), ('phoned', 614), ('full', 615), ('glad', 616), ('okay', 617), ('bless', 618), (\"how's\", 619), ('fainted', 620), ('worked', 621), ('30', 622), ('course', 623), ('quick', 624), ('men', 625), ('poor', 626), ('envy', 627), ('fixed', 628), ('art', 629), ('yes', 630), ('biased', 631), ('eating', 632), ('ruined', 633), ('strict', 634), ('night', 635), ('mind', 636), ('sec', 637), ('cheerful', 638), ('watchful', 639), ('cancel', 640), ('fish', 641), ('changed', 642), ('share', 643), ('dumb', 644), ('milk', 645), ('cake', 646), ('cats', 647), ('soup', 648), ('recovered', 649), ('succeeded', 650), ('manage', 651), ('set', 652), ('nervous', 653), ('talking', 654), ('unhappy', 655), ('lunch', 656), ('timing', 657), ('put', 658), ('bath', 659), ('bite', 660), ('icky', 661), ('hugged', 662), ('refused', 663), ('says', 664), ('meat', 665), ('empty', 666), ('doctor', 667), ('driven', 668), ('exciting', 669), ('believe', 670), ('faint', 671), ('giddy', 672), ('handled', 673), ('plans', 674), ('warned', 675), ('positive', 676), ('seen', 677), ('cancer', 678), ('smiling', 679), ('closely', 680), ('recess', 681), ('someone', 682), ('stir', 683), ('arguing', 684), ('command', 685), ('meant', 686), ('pale', 687), ('mature', 688), ('respectful', 689), ('god', 690), ('boston', 691), ('coffee', 692), ('dinner', 693), ('bragging', 694), ('creepy', 695), (\"we've\", 696), ('cheers', 697), ('bury', 698), ('fold', 699), ('dozed', 700), ('stink', 701), ('long', 702), ('high', 703), ('real', 704), ('tries', 705), ('wise', 706), ('forgot', 707), ('refuse', 708), ('choose', 709), ('fantastic', 710), ('beg', 711), ('cringed', 712), ('giggled', 713), ('hurried', 714), ('loyal', 715), ('far', 716), ('stinks', 717), ('tv', 718), ('seize', 719), ('swam', 720), ('voted', 721), ('agreed', 722), ('waited', 723), ('walked', 724), ('idiot', 725), ('aboard', 726), ('clear', 727), ('destroy', 728), ('fret', 729), ('rush', 730), ('facts', 731), ('lovely', 732), ('flinched', 733), ('air', 734), ('ice', 735), ('screamed', 736), ('squinted', 737), ('survived', 738), ('wrote', 739), ('hero', 740), ('buying', 741), ('chubby', 742), ('paying', 743), ('single', 744), ('happens', 745), ('town', 746), ('lighten', 747), ('alert', 748), ('replace', 749), ('cheat', 750), ('flies', 751), ('winced', 752), ('yelled', 753), ('harder', 754), ('fail', 755), ('hat', 756), ('argue', 757), ('box', 758), ('hated', 759), ('jerk', 760), ('disagreed', 761), ('rice', 762), ('bet', 763), ('struggled', 764), ('ashamed', 765), ('certain', 766), ('dieting', 767), ('leaving', 768), ('shocked', 769), ('sloshed', 770), ('stuffed', 771), ('thirsty', 772), ('through', 773), ('useless', 774), ('wealthy', 775), ('legal', 776), ('doll', 777), ('split', 778), ('wish', 779), ('means', 780), ('card', 781), ('grow', 782), ('shake', 783), ('leg', 784), ('shall', 785), ('mess', 786), ('team', 787), ('whose', 788), ('invited', 789), ('arm', 790), ('attentive', 791), ('rifle', 792), ('bigot', 793), ('pen', 794), ('pathetic', 795), ('always', 796), ('apologized', 797), ('draw', 798), ('map', 799), ('robbed', 800), ('twice', 801), ('sue', 802), ('farmer', 803), ('divorced', 804), ('saint', 805), ('stubborn', 806), ('obvious', 807), ('digging', 808), ('writing', 809), ('quietly', 810), ('sued', 811), ('pushing', 812), ('control', 813), (\"that'll\", 814), ('pig', 815), ('stole', 816), ('today', 817), ('overslept', 818), ('woman', 819), ('sent', 820), ('any', 821), ('supportive', 822), ('pitch', 823), ('dream', 824), ('dry', 825), ('kite', 826), ('ignored', 827), ('beautiful', 828), ('freaked', 829), ('clue', 830), ('winter', 831), ('hiking', 832), ('object', 833), ('ought', 834), ('surrendered', 835), ('trusted', 836), ('stabbed', 837), ('depressed', 838), ('forgetful', 839), ('observant', 840), ('leak', 841), ('takes', 842), ('boring', 843), ('treat', 844), ('outdated', 845), ('minute', 846), ('climbing', 847), ('number', 848), ('insane', 849), ('cup', 850), ('till', 851), ('six', 852), ('wicked', 853), ('hiring', 854), ('wow', 855), ('duck', 856), ('snore', 857), ('low', 858), ('warn', 859), ('answer', 860), ('birds', 861), ('west', 862), ('exhaled', 863), ('grinned', 864), ('pack', 865), ('cop', 866), ('cured', 867), ('sober', 868), ('sand', 869), ('aah', 870), ('walks', 871), ('along', 872), ('hung', 873), ('tragic', 874), ('relented', 875), ('girl', 876), ('amused', 877), ('hiding', 878), ('pooped', 879), ('unwell', 880), ('matters', 881), ('fad', 882), ('fox', 883), ('green', 884), ('phony', 885), ('bus', 886), ('cooked', 887), ('boys', 888), ('even', 889), ('generous', 890), ('sensible', 891), ('resigned', 892), ('strange', 893), ('asked', 894), ('confessed', 895), ('beans', 896), ('bread', 897), ('fruit', 898), ('exercised', 899), ('hesitated', 900), ('nailed', 901), ('protested', 902), ('sell', 903), ('third', 904), ('washed', 905), ('attend', 906), ('correct', 907), ('engaged', 908), ('fasting', 909), ('humming', 910), ('smashed', 911), ('staying', 912), ('unarmed', 913), ('happened', 914), ('simple', 915), ('spring', 916), ('enter', 917), ('clap', 918), ('release', 919), ('return', 920), ('blushed', 921), ('lame', 922), ('huge', 923), ('numb', 924), ('laughed', 925), ('sweated', 926), ('pain', 927), ('pity', 928), ('escaped', 929), ('fall', 930), ('teach', 931), ('morning', 932), ('donut', 933), ('poet', 934), ('annoying', 935), ('horrible', 936), ('romantic', 937), ('almost', 938), ('apples', 939), ('doubts', 940), ('women', 941), ('star', 942), ('slept', 943), ('wonder', 944), ('explain', 945), ('confused', 946), ('grounded', 947), ('hopeless', 948), ('outraged', 949), ('shooting', 950), ('wolf', 951), ('unfair', 952), ('pizza', 953), ('happen', 954), ('waste', 955), ('archaic', 956), ('garbage', 957), ('genuine', 958), (\"school's\", 959), ('things', 960), (\"this'll\", 961), ('approved', 962), ('small', 963), ('polite', 964), ('nerve', 965), (\"you'll\", 966), ('dreaming', 967), ('slipping', 968), ('expert', 969), ('together', 970), ('blow', 971), ('circle', 972), (\"everyone's\", 973), ('exhale', 974), ('smells', 975), ('senior', 976), ('comes', 977), ('or', 978), ('nose', 979), ('muslim', 980), ('became', 981), ('despise', 982), ('queasy', 983), ('elected', 984), ('ring', 985), ('movies', 986), ('copy', 987), ('owe', 988), ('raise', 989), ('funds', 990), ('volunteered', 991), ('allow', 992), ('plumber', 993), ('convinced', 994), ('intrigued', 995), ('resilient', 996), ('voting', 997), ('helping', 998), ('flaw', 999), ('viral', 1000), ('weapon', 1001), ('accurate', 1002), (\"one's\", 1003), ('clearly', 1004), ('symptoms', 1005), ('vary', 1006), ('shower', 1007), ('tastes', 1008), ('differ', 1009), ('pretty', 1010), ('fooled', 1011), ('touchy', 1012), ('remembers', 1013), ('scares', 1014), ('trusts', 1015), ('growing', 1016), ('sinking', 1017), ('morons', 1018), ('qualified', 1019), ('cows', 1020), ('hop', 1021), ('froze', 1022), ('swore', 1023), ('waved', 1024), ('kick', 1025), ('perfect', 1026), ('skip', 1027), ('wash', 1028), ('goofed', 1029), ('jumped', 1030), ('moaned', 1031), ('nodded', 1032), ('sighed', 1033), ('game', 1034), ('tidy', 1035), ('marry', 1036), ('speed', 1037), ('chill', 1038), ('bark', 1039), ('film', 1040), ('hands', 1041), ('dj', 1042), ('sexy', 1043), ('east', 1044), (\"here's\", 1045), ('humor', 1046), ('blinked', 1047), ('groaned', 1048), ('inhaled', 1049), ('promise', 1050), ('tripped', 1051), ('leo', 1052), ('snowed', 1053), ('8', 1054), ('dark', 1055), ('below', 1056), ('unlock', 1057), (\"who'll\", 1058), ('higher', 1059), ('bottoms', 1060), ('breathe', 1061), ('dig', 1062), ('swiss', 1063), ('south', 1064), ('disagree', 1065), ('doubt', 1066), ('exercise', 1067), ('listened', 1068), ('two', 1069), ('panicked', 1070), ('shrugged', 1071), ('whistled', 1072), ('baking', 1073), ('humble', 1074), ('immune', 1075), ('soaked', 1076), ('bogus', 1077), ('cheap', 1078), ('cd', 1079), ('safer', 1080), ('jesus', 1081), ('wept', 1082), ('pray', 1083), ('mama', 1084), ('comment', 1085), ('scoot', 1086), ('nap', 1087), ('notes', 1088), ('gasped', 1089), ('unscrew', 1090), ('sunk', 1091), ('specific', 1092), ('comfort', 1093), ('count', 1094), ('bowl', 1095), ('gloat', 1096), ('laugh', 1097), ('panic', 1098), ('stare', 1099), ('dressed', 1100), ('grew', 1101), ('guts', 1102), ('hunk', 1103), ('nerd', 1104), ('slob', 1105), ('monk', 1106), ('apologize', 1107), ('buried', 1108), ('class', 1109), ('golf', 1110), ('heard', 1111), ('jazz', 1112), ('often', 1113), ('oppose', 1114), ('taxes', 1115), ('cars', 1116), ('snickered', 1117), ('surrender', 1118), ('scream', 1119), ('baker', 1120), ('blessed', 1121), ('chicken', 1122), ('cooking', 1123), ('falling', 1124), ('finicky', 1125), ('frantic', 1126), ('furious', 1127), ('healthy', 1128), ('jealous', 1129), ('popular', 1130), ('psychic', 1131), ('sincere', 1132), ('stumped', 1133), ('foggy', 1134), ('rain', 1135), ('flat', 1136), ('monday', 1137), ('fact', 1138), ('pipe', 1139), ('rule', 1140), ('scam', 1141), ('song', 1142), ('trap', 1143), ('cloudy', 1144), ('futile', 1145), ('ironic', 1146), ('locked', 1147), ('silent', 1148), ('warmer', 1149), ('across', 1150), ('party', 1151), ('talks', 1152), ('plants', 1153), ('press', 1154), ('record', 1155), ('settle', 1156), ('stick', 1157), ('surprise', 1158), ('kissed', 1159), ('drowned', 1160), ('glum', 1161), ('loud', 1162), ('neat', 1163), ('wary', 1164), ('watched', 1165), ('older', 1166), ('caution', 1167), ('evil', 1168), ('meet', 1169), ('bore', 1170), ('dope', 1171), ('drag', 1172), ('next', 1173), ('anything', 1174), ('realistic', 1175), ('beef', 1176), ('ramble', 1177), ('slouch', 1178), ('dress', 1179), ('safely', 1180), ('examine', 1181), ('half', 1182), ('snack', 1183), ('avoids', 1184), ('heroic', 1185), ('barbaric', 1186), ('dare', 1187), ('chess', 1188), ('sugar', 1189), ('books', 1190), ('jokes', 1191), ('cab', 1192), ('paint', 1193), ('rewrote', 1194), ('rose', 1195), ('smelled', 1196), ('understand', 1197), ('beaten', 1198), ('learn', 1199), ('wimped', 1200), ('risk', 1201), ('99', 1202), ('canadian', 1203), ('coward', 1204), ('dancer', 1205), ('friend', 1206), ('lawyer', 1207), ('singer', 1208), ('surfer', 1209), ('bleeding', 1210), ('cultured', 1211), ('diabetic', 1212), ('doing', 1213), ('grateful', 1214), ('homesick', 1215), ('involved', 1216), ('mistaken', 1217), ('offended', 1218), ('powerful', 1219), ('rational', 1220), ('reformed', 1221), ('starving', 1222), ('thrilled', 1223), ('ticklish', 1224), ('blame', 1225), ('unbiased', 1226), ('deer', 1227), ('amazed', 1228), ('vague', 1229), ('curse', 1230), ('rumor', 1231), ('amazing', 1232), ('bedtime', 1233), ('complex', 1234), ('missing', 1235), ('snowing', 1236), ('special', 1237), ('looking', 1238), ('ladies', 1239), ('lead', 1240), ('review', 1241), ('pigs', 1242), ('feet', 1243), ('plug', 1244), ('shrieked', 1245), ('sang', 1246), ('forward', 1247), ('pouting', 1248), ('staring', 1249), ('whining', 1250), ('yawning', 1251), ('dreams', 1252), ('approve', 1253), ('bores', 1254), ('enlisted', 1255), ('grumbled', 1256), ('spy', 1257), ('obese', 1258), ('scary', 1259), ('tense', 1260), ('translate', 1261), ('focus', 1262), ('ufo', 1263), ('doomed', 1264), ('heroes', 1265), ('loser', 1266), ('vanished', 1267), ('ink', 1268), ('imbecile', 1269), ('scare', 1270), ('bossy', 1271), (\"ain't\", 1272), ('futon', 1273), ('included', 1274), ('talented', 1275), ('questions', 1276), ('else', 1277), ('reasonable', 1278), ('blindfold', 1279), ('security', 1280), ('skate', 1281), ('crime', 1282), ('deny', 1283), ('despair', 1284), ('something', 1285), ('peas', 1286), ('feed', 1287), ('bird', 1288), ('clock', 1289), ('coat', 1290), ('downstairs', 1291), ('cookie', 1292), ('ham', 1293), ('cracked', 1294), ('feels', 1295), ('maid', 1296), ('braces', 1297), ('type', 1298), ('skating', 1299), ('killed', 1300), ('pinched', 1301), ('seems', 1302), ('adorable', 1303), ('insecure', 1304), ('studying', 1305), ('bled', 1306), ('line', 1307), ('honk', 1308), ('horn', 1309), ('about', 1310), ('thrilling', 1311), ('borrowed', 1312), ('bribed', 1313), ('survive', 1314), ('caused', 1315), ('contributed', 1316), ('designed', 1317), ('reborn', 1318), ('finally', 1319), ('scammed', 1320), ('visa', 1321), ('wife', 1322), ('asthma', 1323), ('rights', 1324), ('people', 1325), ('camels', 1326), ('poetry', 1327), ('autumn', 1328), ('mom', 1329), ('insist', 1330), ('ride', 1331), ('week', 1332), ('own', 1333), ('horse', 1334), ('prefer', 1335), ('rescheduled', 1336), ('respect', 1337), ('rat', 1338), ('blood', 1339), ('dazzled', 1340), ('drugged', 1341), ('pleased', 1342), ('tempted', 1343), ('teacher', 1344), ('tourist', 1345), ('ambitious', 1346), ('artist', 1347), ('committed', 1348), ('concerned', 1349), ('contented', 1350), ('dedicated', 1351), ('desperate', 1352), ('disgusted', 1353), ('expecting', 1354), ('impatient', 1355), ('impulsive', 1356), ('miserable', 1357), ('motivated', 1358), ('fan', 1359), ('powerless', 1360), ('surprised', 1361), ('terrified', 1362), ('uninsured', 1363), ('wig', 1364), ('gag', 1365), ('gets', 1366), ('worse', 1367), ('sense', 1368), ('test', 1369), ('superb', 1370), ('relief', 1371), ('rental', 1372), ('homemade', 1373), ('stealing', 1374), ('hustling', 1375), ('standing', 1376), ('proceed', 1377), ('little', 1378), ('choice', 1379), ('offer', 1380), ('foot', 1381), ('gambling', 1382), ('peace', 1383), ('bell', 1384), ('seal', 1385), ('model', 1386), ('beside', 1387), ('moment', 1388), ('clapping', 1389), ('shouting', 1390), ('spitting', 1391), ('worrying', 1392), ('straighten', 1393), ('sweep', 1394), ('breath', 1395), ('chance', 1396), ('myth', 1397), ('asian', 1398), ('spies', 1399), ('dvd', 1400), ('throw', 1401), ('crept', 1402), ('family', 1403), ('unhurt', 1404), ('upbeat', 1405), ('winded', 1406), ('joined', 1407), ('misled', 1408), ('rushed', 1409), ('seemed', 1410), ('staggered', 1411), ('arrived', 1412), ('packing', 1413), ('page', 1414), ('second', 1415), ('snoring', 1416), ('wax', 1417), ('floor', 1418), ('rules', 1419), ('before', 1420), ('cousins', 1421), ('enemies', 1422), ('sync', 1423), ('bummer', 1424), ('fiasco', 1425), ('nonsense', 1426), (\"what'll\", 1427), ('when', 1428), ('which', 1429), ('thinks', 1430), ('legibly', 1431), ('charlatan', 1432), ('mail', 1433), ('hoot', 1434), ('snob', 1435), ('add', 1436), ('racist', 1437), (\"aren't\", 1438), ('avoid', 1439), ('clichés', 1440), ('camera', 1441), ('juggle', 1442), ('charge', 1443), ('ball', 1444), ('fled', 1445), ('knit', 1446), ('19', 1447), ('drove', 1448), ('pair', 1449), ('cheer', 1450), ('cuff', 1451), ('guys', 1452), ('deep', 1453), ('cursed', 1454), ('lies', 1455), ('5', 1456), ('cheered', 1457), ('frowned', 1458), ('gloated', 1459), ('grunted', 1460), ('noticed', 1461), ('prepaid', 1462), ('shouted', 1463), ('needy', 1464), ('timid', 1465), ('poured', 1466), ('3', 1467), ('above', 1468), ('then', 1469), (\"time's\", 1470), ('cooks', 1471), ('dived', 1472), ('knits', 1473), ('limps', 1474), ('rocks', 1475), ('carry', 1476), ('calls', 1477), ('fill', 1478), ('north', 1479), ('absurd', 1480), ('admit', 1481), ('human', 1482), ('cpr', 1483), ('r', 1484), ('b', 1485), ('objected', 1486), ('shivered', 1487), ('threw', 1488), ('accept', 1489), ('flabby', 1490), ('thirty', 1491), ('wasted', 1492), ('7', 1493), ('begun', 1494), ('bulky', 1495), ('magic', 1496), ('taboo', 1497), ('chat', 1498), ('lasts', 1499), ('agrees', 1500), ('braked', 1501), ('cheats', 1502), ('cusses', 1503), ('drinks', 1504), ('drives', 1505), ('farted', 1506), ('snores', 1507), ('sobbed', 1508), ('yawned', 1509), ('cds', 1510), ('day', 1511), (\"beer's\", 1512), ('blink', 1513), ('shout', 1514), ('flip', 1515), ('coin', 1516), ('eight', 1517), ('nasty', 1518), ('faking', 1519), ('adore', 1520), ('also', 1521), ('taller', 1522), ('assume', 1523), ('bought', 1524), ('tapes', 1525), ('chose', 1526), ('mice', 1527), ('news', 1528), ('math', 1529), ('rock', 1530), ('glue', 1531), ('lips', 1532), ('gas', 1533), ('17', 1534), ('child', 1535), ('medic', 1536), ('widow', 1537), ('jittery', 1538), ('neutral', 1539), ('psyched', 1540), ('puzzled', 1541), ('selfish', 1542), ('teasing', 1543), ('fatal', 1544), ('snow', 1545), ('50', 1546), ('yen', 1547), ('coup', 1548), ('poison', 1549), ('secret', 1550), ('urgent', 1551), ('closer', 1552), ('lift', 1553), ('paper', 1554), ('burns', 1555), ('curt', 1556), (\"tv's\", 1557), ('coughed', 1558), ('gambles', 1559), ('gargled', 1560), ('guessed', 1561), ('grim', 1562), ('meek', 1563), ('slim', 1564), ('vain', 1565), ('listens', 1566), ('reacted', 1567), ('sneered', 1568), ('sniffed', 1569), ('snorted', 1570), ('hell', 1571), ('dump', 1572), ('heel', 1573), ('loss', 1574), (\"what'd\", 1575), (\"where's\", 1576), ('drew', 1577), ('owns', 1578), ('abandon', 1579), ('ship', 1580), ('guest', 1581), ('backup', 1582), ('describe', 1583), ('litter', 1584), ('warmly', 1585), ('ghosts', 1586), ('exist', 1587), ('evening', 1588), ('spoon', 1589), ('another', 1590), ('cranky', 1591), ('kicked', 1592), ('mocked', 1593), ('hey', 1594), ('arrogant', 1595), ('caviar', 1596), ('chased', 1597), ('easily', 1598), ('deserve', 1599), ('enjoyed', 1600), ('lousy', 1601), ('jogging', 1602), ('carded', 1603), ('ax', 1604), ('liars', 1605), ('hives', 1606), ('honor', 1607), ('improvised', 1608), ('honey', 1609), ('opera', 1610), ('sushi', 1611), ('heart', 1612), ('games', 1613), ('trips', 1614), ('bag', 1615), ('space', 1616), ('piano', 1617), ('rugby', 1618), ('maybe', 1619), ('lion', 1620), ('shoes', 1621), ('suppose', 1622), ('sympathize', 1623), ('thought', 1624), ('risks', 1625), ('understood', 1626), ('canned', 1627), ('framed', 1628), ('weighed', 1629), ('gemini', 1630), ('taurus', 1631), ('barber', 1632), ('father', 1633), ('genius', 1634), ('member', 1635), ('priest', 1636), ('purist', 1637), ('ears', 1638), ('adult', 1639), ('agent', 1640), ('autistic', 1641), ('bluffing', 1642), ('drowning', 1643), ('eighteen', 1644), ('faithful', 1645), ('famished', 1646), ('fearless', 1647), ('freezing', 1648), ('gullible', 1649), ('homeless', 1650), ('hungover', 1651), ('managing', 1652), ('angel', 1653), ('rebel', 1654), ('obedient', 1655), ('pregnant', 1656), ('reliable', 1657), ('reserved', 1658), ('restless', 1659), ('shopping', 1660), ('sleeping', 1661), ('speaking', 1662), ('thinking', 1663), ('truthful', 1664), ('worn', 1665), ('checked', 1666), ('decided', 1667), ('solid', 1668), ('identify', 1669), ('iron', 1670), ('elk', 1671), ('black', 1672), ('noisy', 1673), ('white', 1674), ('heal', 1675), ('cinch', 1676), ('plant', 1677), ('robot', 1678), ('setup', 1679), ('hearsay', 1680), ('hideous', 1681), ('suicide', 1682), ('law', 1683), ('diary', 1684), ('walking', 1685), ('bounce', 1686), ('goes', 1687), ('mull', 1688), ('itch', 1689), ('hip', 1690), ('legs', 1691), ('cared', 1692), ('louder', 1693), ('filming', 1694), ('gawking', 1695), ('smoking', 1696), ('whiff', 1697), ('trash', 1698), ('gold', 1699), ('rare', 1700), ('spam', 1701), ('approves', 1702), ('followed', 1703), ('insisted', 1704), ('pudgy', 1705), ('stoic', 1706), ('vegan', 1707), ('sniffled', 1708), ('stumbled', 1709), ('stutters', 1710), ('uneasy', 1711), ('applauded', 1712), ('arabs', 1713), ('jobs', 1714), ('starve', 1715), ('adults', 1716), ('closed', 1717), ('dating', 1718), ('minors', 1719), ('shock', 1720), ('words', 1721), ('matter', 1722), ('years', 1723), ('passed', 1724), ('amuse', 1725), ('gross', 1726), ('moody', 1727), ('beware', 1728), ('body', 1729), ('drown', 1730), ('gamble', 1731), ('mock', 1732), ('sass', 1733), ('healthily', 1734), ('dies', 1735), ('wins', 1736), ('deeply', 1737), ('flowers', 1738), ('bloom', 1739), ('haircut', 1740), ('few', 1741), ('riddance', 1742), ('advises', 1743), ('dropped', 1744), ('dug', 1745), ('hole', 1746), ('eats', 1747), ('british', 1748), ('english', 1749), ('thief', 1750), ('foolish', 1751), ('toys', 1752), ('sells', 1753), ('american', 1754), ('grouch', 1755), ('jesuit', 1756), ('tycoon', 1757), ('demented', 1758), ('rope', 1759), ('delicious', 1760), ('acquired', 1761), ('added', 1762), ('admired', 1763), ('already', 1764), ('japanese', 1765), ('runner', 1766), ('baked', 1767), ('pie', 1768), ('believed', 1769), ('belong', 1770), ('cough', 1771), ('classes', 1772), ('deserved', 1773), ('part', 1774), ('dislike', 1775), ('steal', 1776), ('dye', 1777), ('hair', 1778), ('bananas', 1779), ('poker', 1780), ('exaggerated', 1781), ('drowsy', 1782), ('badly', 1783), ('forbid', 1784), ('fought', 1785), ('bonus', 1786), ('fleeced', 1787), ('fever', 1788), ('crowds', 1789), ('flying', 1790), ('soccer', 1791), ('orders', 1792), ('tenure', 1793), ('sirens', 1794), ('note', 1795), ('cheese', 1796), ('cities', 1797), ('clocks', 1798), ('garlic', 1799), ('grapes', 1800), ('prunes', 1801), ('salmon', 1802), ('trains', 1803), ('yellow', 1804), ('sock', 1805), ('crepes', 1806), ('horses', 1807), ('nature', 1808), ('summer', 1809), ('nearly', 1810), ('crew', 1811), ('lamp', 1812), ('taxi', 1813), ('advice', 1814), ('yacht', 1815), ('hockey', 1816), ('squash', 1817), ('violin', 1818), ('played', 1819), ('quite', 1820), ('resent', 1821), ('ghost', 1822), ('plane', 1823), ('light', 1824), ('queen', 1825), ('bacon', 1826), ('support', 1827), ('tied', 1828), ('firefox', 1829), ('pony', 1830), ('pool', 1831), ('crushed', 1832), ('behave', 1833), ('budge', 1834), ('wore', 1835), ('mask', 1836), ('nights', 1837), ('poems', 1838), ('songs', 1839), ('prove', 1840), ('scorpio', 1841), ('dentist', 1842), ('soldier', 1843), ('student', 1844), ('surgeon', 1845), ('trainee', 1846), ('widower', 1847), ('adaptable', 1848), ('orphan', 1849), ('available', 1850), ('bilingual', 1851), ('delighted', 1852), ('different', 1853), ('easygoing', 1854), ('efficient', 1855), ('flattered', 1856), ('important', 1857), ('impressed', 1858), ('danger', 1859), ('listening', 1860), ('diet', 1861), ('parole', 1862), ('strike', 1863), ('plastered', 1864), ('resentful', 1865), ('satisfied', 1866), ('saying', 1867), ('screaming', 1868), ('sensitive', 1869), ('shivering', 1870), ('skeptical', 1871), ('surviving', 1872), ('coach', 1873), ('unmarried', 1874), ('damaged', 1875), ('leather', 1876), ('painful', 1877), ('raining', 1878), ('bat', 1879), ('apart', 1880), ('sickens', 1881), ('bomb', 1882), ('gift', 1883), ('hoax', 1884), ('worries', 1885), ('saturday', 1886), ('parody', 1887), ('sequel', 1888), ('business', 1889), ('gorgeous', 1890), ('indecent', 1891), ('instinct', 1892), ('midnight', 1893), ('fault', 1894), ('2', 1895), ('obsolete', 1896), ('occupied', 1897), ('personal', 1898), ('possible', 1899), ('suicidal', 1900), ('unlikely', 1901), ('unlocked', 1902), ('worth', 1903), ('paddling', 1904), ('mortal', 1905), ('motion', 1906), ('aches', 1907), (\"hair's\", 1908), ('lungs', 1909), (\"name's\", 1910), (\"nobody's\", 1911), ('window', 1912), ('cage', 1913), ('attention', 1914), ('phone', 1915), ('aloud', 1916), ('russia', 1917), ('later', 1918), ('table', 1919), ('active', 1920), ('obeys', 1921), ('livid', 1922), ('cutie', 1923), ('near', 1924), ('ease', 1925), ('frowning', 1926), ('laughing', 1927), ('scowling', 1928), ('stuff', 1929), ('suit', 1930), ('heap', 1931), ('anyway', 1932), ('suits', 1933), ('saturn', 1934), ('boat', 1935), ('tree', 1936), ('doable', 1937), ('rad', 1938), ('unreal', 1939), ('untrue', 1940), ('cow', 1941), ('mooed', 1942), (\"oven's\", 1943), ('embraced', 1944), ('japan', 1945), ('pun', 1946), ('basic', 1947), ('false', 1948), ('adores', 1949), ('exercises', 1950), ('graduated', 1951), ('goats', 1952), ('sheep', 1953), ('hick', 1954), ('online', 1955), ('sexist', 1956), ('somber', 1957), ('stable', 1958), ('overdosed', 1959), ('remarried', 1960), ('shuddered', 1961), ('stiffened', 1962), ('tensed', 1963), ('testified', 1964), ('texted', 1965), ('frank', 1966), ('would', 1967), ('adopted', 1968), ('annoyed', 1969), ('choking', 1970), ('elderly', 1971), ('injured', 1972), ('violent', 1973), ('wounded', 1974), ('untie', 1975), ('assumed', 1976), ('kept', 1977), ('trees', 1978), ('tools', 1979), ('opposed', 1980), ('candy', 1981), ('prevail', 1982), ('rebuild', 1983), ('respond', 1984), ('succeed', 1985), ('group', 1986), ('anxious', 1987), ('buddies', 1988), ('doctors', 1989), ('lawyers', 1990), ('related', 1991), ('stalled', 1992), ('waiting', 1993), ('winners', 1994), ('matured', 1995), ('hassle', 1996), (\"when's\", 1997), ('disagrees', 1998), ('absent', 1999), ('responded', 2000), ('hypocrite', 2001), (\"you'd\", 2002), ('idiots', 2003), ('unkind', 2004), ('coke', 2005), ('intruding', 2006), ('yourselves', 2007), ('boats', 2008), ('sink', 2009), ('shovel', 2010), ('gate', 2011), ('contact', 2012), ('file', 2013), ('grass', 2014)])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2idx_inputs.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c7b6cde1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique words in the output: 5641\n",
      "Length of longest sentence in the output: 11\n"
     ]
    }
   ],
   "source": [
    "## it is output similar as input\n",
    "\n",
    "output_tokenizer = Tokenizer(num_words=MAX_NUM_WORDS, filters='')\n",
    "output_tokenizer.fit_on_texts(output_sentences + output_sentences_inputs)\n",
    "output_integer_seq = output_tokenizer.texts_to_sequences(output_sentences)\n",
    "output_input_integer_seq = output_tokenizer.texts_to_sequences(output_sentences_inputs)\n",
    "\n",
    "word2idx_outputs = output_tokenizer.word_index\n",
    "print('Total unique words in the output: %s' % len(word2idx_outputs))\n",
    "\n",
    "num_words_output = len(word2idx_outputs) + 1\n",
    "max_out_len = max(len(sen) for sen in output_integer_seq)\n",
    "print(\"Length of longest sentence in the output: %g\" % max_out_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "875f4131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_input_sequences.shape: (10000, 4)\n",
      "encoder_input_sequences[172]: [  0   0  15 196]\n"
     ]
    }
   ],
   "source": [
    "##padding the input and the output is that text sentences can be of varying length, however LSTM (the algorithm that we are going to train our model) expects input instances with the same length\n",
    "encoder_input_sequences = pad_sequences(input_integer_seq, maxlen=max_input_len)\n",
    "print(\"encoder_input_sequences.shape:\", encoder_input_sequences.shape)\n",
    "print(\"encoder_input_sequences[172]:\", encoder_input_sequences[172])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b3fdc53e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "313\n"
     ]
    }
   ],
   "source": [
    "print(word2idx_inputs[\"i'm\"])\n",
    "print(word2idx_inputs[\"ill\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a0999ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_input_sequences.shape: (10000, 11)\n",
      "decoder_input_sequences[172]: [  2  45 604   0   0   0   0   0   0   0   0]\n",
      "[[  44    4    1 ...    0    0    0]\n",
      " [ 420    1    0 ...    0    0    0]\n",
      " [  22  330    4 ...    0    0    0]\n",
      " ...\n",
      " [ 165  132  106 ...    0    0    0]\n",
      " [  11  165 2394 ...    0    0    0]\n",
      " [  11  165 5641 ...    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "## same for output\n",
    "decoder_input_sequences = pad_sequences(output_input_integer_seq, maxlen=max_out_len, padding='post')\n",
    "print(\"decoder_input_sequences.shape:\", decoder_input_sequences.shape)\n",
    "print(\"decoder_input_sequences[172]:\", decoder_input_sequences[172])\n",
    "decoder_output_sequences = pad_sequences(output_integer_seq, maxlen=max_out_len, padding='post')\n",
    "#print(decoder_output_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "557270e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "6\n",
      "137\n"
     ]
    }
   ],
   "source": [
    "print(word2idx_outputs[\"<sos>\"])\n",
    "print(word2idx_outputs[\"je\"])\n",
    "print(word2idx_outputs[\"suis\"])\n",
    "print(word2idx_outputs[\"malade.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6bbfcaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Create word Embedding for I/P i.e. English, we are using GLoVe embedding, for french we will be using custom embedding\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "\n",
    "embeddings_dictionary = dict()\n",
    "\n",
    "glove_file = open(r'E:\\Pycharm_Projects\\tf_prac\\glove.6B\\glove.6B.100d.txt', encoding=\"utf8\")\n",
    "\n",
    "for line in glove_file:\n",
    "    records = line.split()\n",
    "    word = records[0]\n",
    "    vector_dimensions = asarray(records[1:], dtype='float32')\n",
    "    embeddings_dictionary[word] = vector_dimensions\n",
    "glove_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404318c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "90485516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "num_words = min(MAX_NUM_WORDS, len(word2idx_inputs) + 1)\n",
    "print(num_words)\n",
    "\n",
    "embedding_matrix = zeros((num_words, EMBEDDING_SIZE))\n",
    "print(embedding_matrix)\n",
    "for word, index in word2idx_inputs.items():\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "640874e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.12648    0.1366     0.22192   -0.025204  -0.7197     0.66147\n",
      "  0.48509    0.057223   0.13829   -0.26375   -0.23647    0.74349\n",
      "  0.46737   -0.462      0.20031   -0.26302    0.093948  -0.61756\n",
      " -0.28213    0.1353     0.28213    0.21813    0.16418    0.22547\n",
      " -0.98945    0.29624   -0.62476   -0.29535    0.21534    0.92274\n",
      "  0.38388    0.55744   -0.14628   -0.15674   -0.51941    0.25629\n",
      " -0.0079678  0.12998   -0.029192   0.20868   -0.55127    0.075353\n",
      "  0.44746   -0.71046    0.75562    0.010378   0.095229   0.16673\n",
      "  0.22073   -0.46562   -0.10199   -0.80386    0.45162    0.45183\n",
      "  0.19869   -1.6571     0.7584    -0.40298    0.82426   -0.386\n",
      "  0.0039546  0.61318    0.02701   -0.3308    -0.095652  -0.082164\n",
      "  0.7858     0.13394   -0.32715   -0.31371   -0.20247   -0.73001\n",
      " -0.49343    0.56445    0.61038    0.36777   -0.070182   0.44859\n",
      " -0.61774   -0.18849    0.65592    0.44797   -0.10469    0.62512\n",
      " -1.9474    -0.60622    0.073874   0.50013   -1.1278    -0.42066\n",
      " -0.37322   -0.50538    0.59171    0.46534   -0.42482    0.83265\n",
      "  0.081548  -0.44147   -0.084311  -1.2304   ]\n"
     ]
    }
   ],
   "source": [
    "print(embeddings_dictionary[\"ill\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b9a38c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.16237999 -0.3373     -0.24415    -0.036053    0.12717     0.41297001\n",
      "  0.15265     0.23246001 -0.42772999 -0.71423     0.69331998  0.16859999\n",
      "  0.39746001  0.14218999 -0.31621    -0.72087997 -0.16003001  0.17733\n",
      " -0.14495    -0.26379001  0.67311001  0.90779001  0.52047002 -0.13696\n",
      " -0.31428999 -0.71758002 -0.14982    -0.70389003  0.44791001  0.073398\n",
      " -0.033242    0.14199001  0.34531     0.65033001  0.010394    0.94755\n",
      "  0.058567   -0.0047846   0.17679    -0.34283999 -0.31970999 -0.37520999\n",
      "  0.75291997 -0.1035      0.45971    -0.36877999  0.11353     0.53530997\n",
      "  1.04229999 -1.10889995  0.19391    -0.69931     0.11326     0.90522999\n",
      "  0.12302    -1.81009996 -0.28229001 -0.42172     0.71376997  0.70547998\n",
      " -0.068874    0.31896999 -0.54651999 -0.13504     0.17066     0.24518\n",
      " -0.12982     0.053308   -0.41339001  0.47496    -0.21569     0.0032189\n",
      "  0.17123    -0.063068   -0.1067      0.26807001 -0.12807    -0.21364\n",
      "  0.19554    -0.24145     0.050128    0.2931     -0.66754001  0.13753\n",
      " -0.71947998 -0.43059    -0.20476     0.058859   -0.49169999  0.031449\n",
      " -0.21991    -0.16921     0.54150999  0.32999    -1.08459997  0.20554\n",
      "  0.073388    0.66930997 -0.10954    -0.18932   ]\n"
     ]
    }
   ],
   "source": [
    "print(embedding_matrix[539])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "15d1ad23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creare the embedding layer for I/P\n",
    "embedding_layer = Embedding(num_words, EMBEDDING_SIZE, weights=[embedding_matrix], input_length=max_input_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1358439a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 11, 5642)\n"
     ]
    }
   ],
   "source": [
    "decoder_targets_one_hot = np.zeros((\n",
    "        len(input_sentences),\n",
    "        max_out_len,\n",
    "        num_words_output\n",
    "    ),\n",
    "    dtype='float32'\n",
    ")\n",
    "print(decoder_targets_one_hot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c972e47c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0d5ea93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, d in enumerate(decoder_output_sequences):\n",
    "    for t, word in enumerate(d):\n",
    "        decoder_targets_one_hot[i, t, word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8b0965ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next, we need to create the encoder and decoders. The input to the encoder will be the sentence in English and the output will be the hidden state and cell state of the LSTM.\n",
    "encoder_inputs_placeholder = Input(shape=(max_input_len,))\n",
    "x = embedding_layer(encoder_inputs_placeholder)\n",
    "encoder = LSTM(LSTM_NODES, return_state=True)\n",
    "\n",
    "encoder_outputs, h, c = encoder(x)\n",
    "encoder_states = [h, c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a31f4ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The next step is to define the decoder. The decoder will have two inputs: the hidden state and cell state from the encoder and the input sentence\n",
    "decoder_inputs_placeholder = Input(shape=(max_out_len,))\n",
    "\n",
    "decoder_embedding = Embedding(num_words_output, LSTM_NODES)\n",
    "decoder_inputs_x = decoder_embedding(decoder_inputs_placeholder)\n",
    "\n",
    "decoder_lstm = LSTM(LSTM_NODES, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs_x, initial_state=encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7f161167",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finally, the output from the decoder LSTM is passed through a dense layer to predict decoder outputs\n",
    "decoder_dense = Dense(num_words_output, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "69992bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The next step is to compile the model:\n",
    "model = Model([encoder_inputs_placeholder,\n",
    "  decoder_inputs_placeholder], decoder_outputs)\n",
    "model.compile(\n",
    "    optimizer='rmsprop',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9383f581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 4)]          0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 11)]         0           []                               \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, 4, 100)       201500      ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " embedding_2 (Embedding)        (None, 11, 256)      1444352     ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    [(None, 256),        365568      ['embedding_1[0][0]']            \n",
      "                                 (None, 256),                                                     \n",
      "                                 (None, 256)]                                                     \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  [(None, 11, 256),    525312      ['embedding_2[0][0]',            \n",
      "                                 (None, 256),                     'lstm[0][1]',                   \n",
      "                                 (None, 256)]                     'lstm[0][2]']                   \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 11, 5642)     1449994     ['lstm_1[0][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,986,726\n",
      "Trainable params: 3,986,726\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8921d529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "141/141 [==============================] - 81s 574ms/step - loss: 2.2471 - accuracy: 0.6724 - val_loss: 2.2581 - val_accuracy: 0.6797\n",
      "Epoch 2/20\n",
      "141/141 [==============================] - 65s 459ms/step - loss: 1.9073 - accuracy: 0.7332 - val_loss: 2.1356 - val_accuracy: 0.6808\n",
      "Epoch 3/20\n",
      "141/141 [==============================] - 72s 505ms/step - loss: 1.8013 - accuracy: 0.7424 - val_loss: 2.0444 - val_accuracy: 0.7065\n",
      "Epoch 4/20\n",
      "141/141 [==============================] - 48s 341ms/step - loss: 1.6911 - accuracy: 0.7510 - val_loss: 1.9335 - val_accuracy: 0.7129\n",
      "Epoch 5/20\n",
      "141/141 [==============================] - 46s 329ms/step - loss: 1.5932 - accuracy: 0.7601 - val_loss: 1.8369 - val_accuracy: 0.7267\n",
      "Epoch 6/20\n",
      "141/141 [==============================] - 48s 339ms/step - loss: 1.5122 - accuracy: 0.7684 - val_loss: 1.7870 - val_accuracy: 0.7348\n",
      "Epoch 7/20\n",
      "141/141 [==============================] - 46s 326ms/step - loss: 1.4428 - accuracy: 0.7775 - val_loss: 1.7278 - val_accuracy: 0.7541\n",
      "Epoch 8/20\n",
      "141/141 [==============================] - 50s 355ms/step - loss: 1.3805 - accuracy: 0.7870 - val_loss: 1.6715 - val_accuracy: 0.7604\n",
      "Epoch 9/20\n",
      "141/141 [==============================] - 60s 426ms/step - loss: 1.3268 - accuracy: 0.7953 - val_loss: 1.6373 - val_accuracy: 0.7682\n",
      "Epoch 10/20\n",
      "141/141 [==============================] - 96s 683ms/step - loss: 1.2786 - accuracy: 0.8008 - val_loss: 1.6038 - val_accuracy: 0.7743\n",
      "Epoch 11/20\n",
      "141/141 [==============================] - 92s 648ms/step - loss: 1.2365 - accuracy: 0.8059 - val_loss: 1.5865 - val_accuracy: 0.7726\n",
      "Epoch 12/20\n",
      "141/141 [==============================] - 65s 463ms/step - loss: 1.1981 - accuracy: 0.8100 - val_loss: 1.5744 - val_accuracy: 0.7757\n",
      "Epoch 13/20\n",
      "141/141 [==============================] - 69s 490ms/step - loss: 1.1638 - accuracy: 0.8126 - val_loss: 1.5521 - val_accuracy: 0.7834\n",
      "Epoch 14/20\n",
      "141/141 [==============================] - 43s 306ms/step - loss: 1.1327 - accuracy: 0.8160 - val_loss: 1.5340 - val_accuracy: 0.7853\n",
      "Epoch 15/20\n",
      "141/141 [==============================] - 45s 318ms/step - loss: 1.1031 - accuracy: 0.8180 - val_loss: 1.5214 - val_accuracy: 0.7853\n",
      "Epoch 16/20\n",
      "141/141 [==============================] - 43s 307ms/step - loss: 1.0752 - accuracy: 0.8203 - val_loss: 1.5115 - val_accuracy: 0.7869\n",
      "Epoch 17/20\n",
      "141/141 [==============================] - 43s 304ms/step - loss: 1.0487 - accuracy: 0.8229 - val_loss: 1.5039 - val_accuracy: 0.7875\n",
      "Epoch 18/20\n",
      "141/141 [==============================] - 45s 319ms/step - loss: 1.0242 - accuracy: 0.8249 - val_loss: 1.4879 - val_accuracy: 0.7899\n",
      "Epoch 19/20\n",
      "141/141 [==============================] - 47s 331ms/step - loss: 0.9995 - accuracy: 0.8273 - val_loss: 1.4885 - val_accuracy: 0.7898\n",
      "Epoch 20/20\n",
      "141/141 [==============================] - 46s 329ms/step - loss: 0.9762 - accuracy: 0.8296 - val_loss: 1.4795 - val_accuracy: 0.7893\n"
     ]
    }
   ],
   "source": [
    "r = model.fit(\n",
    "    [encoder_input_sequences, decoder_input_sequences],\n",
    "    decoder_targets_one_hot,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_split=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9b273f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs_placeholder, encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c62e9d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_state_input_h = Input(shape=(LSTM_NODES,))\n",
    "decoder_state_input_c = Input(shape=(LSTM_NODES,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "de3d9503",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs_single = Input(shape=(1,))\n",
    "decoder_inputs_single_x = decoder_embedding(decoder_inputs_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9ecb1e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_outputs, h, c = decoder_lstm(decoder_inputs_single_x, initial_state=decoder_states_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "498a9b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_states = [h, c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "125f7ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_model = Model(\n",
    "    [decoder_inputs_single] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bc20c44a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(decoder_model, to_file='model_plot_dec.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "967e246b",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word_input = {v:k for k, v in word2idx_inputs.items()}\n",
    "idx2word_target = {v:k for k, v in word2idx_outputs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0f8a14fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(input_seq):\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    target_seq[0, 0] = word2idx_outputs['<sos>']\n",
    "    eos = word2idx_outputs['<eos>']\n",
    "    output_sentence = []\n",
    "\n",
    "    for _ in range(max_out_len):\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "        idx = np.argmax(output_tokens[0, 0, :])\n",
    "\n",
    "        if eos == idx:\n",
    "            break\n",
    "\n",
    "        word = ''\n",
    "\n",
    "        if idx > 0:\n",
    "            word = idx2word_target[idx]\n",
    "            output_sentence.append(word)\n",
    "\n",
    "        target_seq[0, 0] = idx\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return ' '.join(output_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9214d24b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 3s 3s/step\n",
      "1/1 [==============================] - 1s 795ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "-\n",
      "Input: I'm relaxed.\n",
      "Response: je suis en train !\n"
     ]
    }
   ],
   "source": [
    "i = np.random.choice(len(input_sentences))\n",
    "input_seq = encoder_input_sequences[i:i+1]\n",
    "translation = translate_sentence(input_seq)\n",
    "print('-')\n",
    "print('Input:', input_sentences[i])\n",
    "print('Response:', translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6598c32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98eed7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
