{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a14846e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = \"eng-fra/fra.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33cb2dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "import re\n",
    "\n",
    "# Convert the unicode sequence to ascii\n",
    "def unicode_to_ascii(s):\n",
    "\n",
    "  # Normalize the unicode string and remove the non-spacking mark\n",
    "  return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "      if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "# Preprocess the sequence\n",
    "def preprocess_sentence(w):\n",
    "\n",
    "  # Clean the sequence\n",
    "  w = unicode_to_ascii(w.lower().strip())\n",
    "\n",
    "  # Create a space between word and the punctuation following it\n",
    "  w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "  w = re.sub(r'[\" \"]+', \" \", w)\n",
    "\n",
    "  # Replace everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "  w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "\n",
    "  w = w.strip()\n",
    "\n",
    "  # Add a start and stop token to detect the start and end of the sequence\n",
    "  w = '<start> ' + w + ' <end>'\n",
    "  return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e8d57ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "# Create the Dataset\n",
    "def create_dataset(path, num_examples):\n",
    "  lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
    "\n",
    "  # Loop through lines (sequences) and extract the English and French sequences. Store them as a word-pair\n",
    "  word_pairs = [[preprocess_sentence(w) for w in l.split('\\t', 2)[:-1]]  for l in lines[:num_examples]]\n",
    "  return zip(*word_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de1c504b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> it may be impossible to get a completely error free corpus due to the nature of this kind of collaborative effort . however , if we encourage members to contribute sentences in their own languages rather than experiment in languages they are learning , we might be able to minimize errors . <end>\n",
      "<start> il est peut etre impossible d obtenir un corpus completement denue de fautes , etant donnee la nature de ce type d entreprise collaborative . cependant , si nous encourageons les membres a produire des phrases dans leurs propres langues plutot que d experimenter dans les langues qu ils apprennent , nous pourrions etre en mesure de reduire les erreurs . <end>\n"
     ]
    }
   ],
   "source": [
    "en, fra = create_dataset(path_to_file, None)\n",
    "print(en[-1])\n",
    "print(fra[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5db96d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Convert sequences to tokenizers\n",
    "def tokenize(lang):\n",
    "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "      filters='')\n",
    "  \n",
    "  # Convert sequences into internal vocab\n",
    "  lang_tokenizer.fit_on_texts(lang)\n",
    "\n",
    "  # Convert internal vocab to numbers\n",
    "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "\n",
    "  # Pad the tensors to assign equal length to all the sequences\n",
    "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
    "                                                         padding='post')\n",
    "\n",
    "  return tensor, lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ac88be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "def load_dataset(path, num_examples=None):\n",
    " \n",
    "  # Create dataset (targ_lan = English, inp_lang = French)\n",
    "  targ_lang, inp_lang = create_dataset(path, num_examples)\n",
    "\n",
    "  # Tokenize the sequences\n",
    "  input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
    "  target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
    "\n",
    "  return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3a3fb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider 50k examples\n",
    "num_examples = 10000\n",
    "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path_to_file, num_examples)\n",
    "\n",
    "# Calculate max_length of the target tensors\n",
    "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c2ac8e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000 8000 2000 2000\n"
     ]
    }
   ],
   "source": [
    "#!pip3 install sklearn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create training and validation sets using an 80/20 split\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
    "\n",
    "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74e73836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Language; index to word mapping\n",
      "1 ----> <start>\n",
      "356 ----> abandonne\n",
      "5 ----> !\n",
      "2 ----> <end>\n",
      "\n",
      "Target Language; index to word mapping\n",
      "1 ----> <start>\n",
      "27 ----> let\n",
      "5 ----> it\n",
      "17 ----> go\n",
      "3 ----> .\n",
      "2 ----> <end>\n"
     ]
    }
   ],
   "source": [
    "# Show the mapping b/w word index and language tokenizer\n",
    "def convert(lang, tensor):\n",
    "  for t in tensor:\n",
    "    if t != 0:\n",
    "      print (\"%d ----> %s\" % (t, lang.index_word[t]))\n",
    "      \n",
    "print (\"Input Language; index to word mapping\")\n",
    "convert(inp_lang, input_tensor_train[0])\n",
    "print ()\n",
    "print (\"Target Language; index to word mapping\")\n",
    "convert(targ_lang, target_tensor_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32a936b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential model parameters\n",
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "vocab_inp_size = len(inp_lang.word_index) + 1\n",
    "vocab_tar_size = len(targ_lang.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85715bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f1bf0da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([64, 15]), TensorShape([64, 8]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Size of input and target batches\n",
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90a661fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder class\n",
    "class Encoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "    super(Encoder, self).__init__()\n",
    "    self.batch_sz = batch_sz\n",
    "    self.enc_units = enc_units\n",
    "\n",
    "    # Embed the vocab to a dense embedding \n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "    # GRU Layer\n",
    "    # glorot_uniform: Initializer for the recurrent_kernel weights matrix, \n",
    "    # used for the linear transformation of the recurrent state\n",
    "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "\n",
    "  # Encoder network comprises an Embedding layer followed by a GRU layer\n",
    "  def call(self, x, hidden):\n",
    "    x = self.embedding(x)\n",
    "    output, state = self.gru(x, initial_state=hidden)\n",
    "    return output, state\n",
    "\n",
    "  # To initialize the hidden state\n",
    "  def initialize_hidden_state(self):\n",
    "    return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f789428f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: (batch size, sequence length, units) (64, 15, 1024)\n",
      "Encoder Hidden state shape: (batch size, units) (64, 1024)\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
    "\n",
    "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "adbd7b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention Mechanism\n",
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, units):\n",
    "    super(BahdanauAttention, self).__init__()\n",
    "    self.W1 = tf.keras.layers.Dense(units)\n",
    "    self.W2 = tf.keras.layers.Dense(units)\n",
    "    self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "  def call(self, query, values):\n",
    "    # query hidden state shape == (batch_size, hidden size)\n",
    "    # values shape == (batch_size, max_len, hidden size)\n",
    "\n",
    "    # we are doing this to broadcast addition along the time axis to calculate the score\n",
    "    # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "    query_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "    # score shape == (batch_size, max_length, 1)\n",
    "    # we get 1 at the last axis because we are applying score to self.V\n",
    "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "    score = self.V(tf.nn.tanh(\n",
    "        self.W1(query_with_time_axis) + self.W2(values)))\n",
    "\n",
    "    # attention_weights shape == (batch_size, max_length, 1)\n",
    "    attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "    # context_vector shape after sum == (batch_size, hidden_size)\n",
    "    context_vector = attention_weights * values\n",
    "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "    return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "229c38d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention result shape: (batch size, units) (64, 1024)\n",
      "Attention weights shape: (batch_size, sequence_length, 1) (64, 15, 1)\n"
     ]
    }
   ],
   "source": [
    "attention_layer = BahdanauAttention(10)\n",
    "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
    "\n",
    "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
    "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa6dfb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder class\n",
    "class Decoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "    super(Decoder, self).__init__()\n",
    "    self.batch_sz = batch_sz\n",
    "    self.dec_units = dec_units\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    # Used for attention\n",
    "    self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "  def call(self, x, hidden, enc_output):\n",
    "    # x shape == (batch_size, 1)\n",
    "    # hidden shape == (batch_size, max_length)\n",
    "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "\n",
    "    # context_vector shape == (batch_size, hidden_size)\n",
    "    # attention_weights shape == (batch_size, max_length, 1)\n",
    "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "\n",
    "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "    x = self.embedding(x)\n",
    "\n",
    "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "    # passing the concatenated vector to the GRU\n",
    "    output, state = self.gru(x)\n",
    "\n",
    "    # output shape == (batch_size * 1, hidden_size)\n",
    "    output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "    # output shape == (batch_size, vocab)\n",
    "    x = self.fc(output)\n",
    "\n",
    "    return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8cea1a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape: (batch_size, vocab size) (64, 1976)\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
    "                                      sample_hidden, sample_output)\n",
    "\n",
    "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d561026a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize optimizer and loss functions\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "# Loss function\n",
    "def loss_function(real, pred):\n",
    "\n",
    "  # Take care of the padding. Not all sequences are of equal length.\n",
    "  # If there's a '0' in the sequence, the loss is being nullified\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "\n",
    "  return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "996b0dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cbb454d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "  loss = 0\n",
    "\n",
    "  # tf.GradientTape() -- record operations for automatic differentiation\n",
    "  with tf.GradientTape() as tape:\n",
    "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "\n",
    "    # dec_hidden is used by attention, hence is the same enc_hidden\n",
    "    dec_hidden = enc_hidden\n",
    "\n",
    "    # <start> token is the initial decoder input\n",
    "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "    # Teacher forcing - feeding the target as the next input\n",
    "    for t in range(1, targ.shape[1]):\n",
    "\n",
    "      # Pass enc_output to the decoder\n",
    "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "\n",
    "      # Compute the loss\n",
    "      loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "      # Use teacher forcing\n",
    "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "  # As this function is called per batch, compute the batch_loss\n",
    "  batch_loss = (loss / int(targ.shape[1]))\n",
    "\n",
    "  # Get the model's variables\n",
    "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "  # Compute the gradients\n",
    "  gradients = tape.gradient(loss, variables)\n",
    "\n",
    "  # Update the variables of the model/network\n",
    "  optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "  return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c223cdbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 1.5867\n",
      "Epoch 1 Batch 100 Loss 1.2273\n",
      "Epoch 1 Loss 1.3791\n",
      "Time taken for 1 epoch 317.83372378349304 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 1.1582\n",
      "Epoch 2 Batch 100 Loss 1.1330\n",
      "Epoch 2 Loss 1.1484\n",
      "Time taken for 1 epoch 307.5996618270874 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 0.9721\n",
      "Epoch 3 Batch 100 Loss 1.0090\n",
      "Epoch 3 Loss 0.9730\n",
      "Time taken for 1 epoch 297.3402404785156 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 0.7923\n",
      "Epoch 4 Batch 100 Loss 0.8283\n",
      "Epoch 4 Loss 0.8130\n",
      "Time taken for 1 epoch 301.7323422431946 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.6574\n",
      "Epoch 5 Batch 100 Loss 0.6709\n",
      "Epoch 5 Loss 0.6564\n",
      "Time taken for 1 epoch 296.256765127182 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 0.4989\n",
      "Epoch 6 Batch 100 Loss 0.5520\n",
      "Epoch 6 Loss 0.5180\n",
      "Time taken for 1 epoch 303.64567613601685 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 0.3774\n",
      "Epoch 7 Batch 100 Loss 0.4263\n",
      "Epoch 7 Loss 0.4057\n",
      "Time taken for 1 epoch 297.38442516326904 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 0.2842\n",
      "Epoch 8 Batch 100 Loss 0.2646\n",
      "Epoch 8 Loss 0.3065\n",
      "Time taken for 1 epoch 302.17978382110596 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.2330\n",
      "Epoch 9 Batch 100 Loss 0.2174\n",
      "Epoch 9 Loss 0.2257\n",
      "Time taken for 1 epoch 295.35170888900757 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.1636\n",
      "Epoch 10 Batch 100 Loss 0.2059\n",
      "Epoch 10 Loss 0.1757\n",
      "Time taken for 1 epoch 302.9428970813751 sec\n",
      "\n",
      "Epoch 11 Batch 0 Loss 0.1030\n",
      "Epoch 11 Batch 100 Loss 0.1212\n",
      "Epoch 11 Loss 0.1411\n",
      "Time taken for 1 epoch 300.1086235046387 sec\n",
      "\n",
      "Epoch 12 Batch 0 Loss 0.0570\n",
      "Epoch 12 Batch 100 Loss 0.1073\n",
      "Epoch 12 Loss 0.1105\n",
      "Time taken for 1 epoch 311.79526686668396 sec\n",
      "\n",
      "Epoch 13 Batch 0 Loss 0.1285\n",
      "Epoch 13 Batch 100 Loss 0.1119\n",
      "Epoch 13 Loss 0.0969\n",
      "Time taken for 1 epoch 309.4544563293457 sec\n",
      "\n",
      "Epoch 14 Batch 0 Loss 0.0716\n",
      "Epoch 14 Batch 100 Loss 0.0931\n",
      "Epoch 14 Loss 0.0860\n",
      "Time taken for 1 epoch 318.1915638446808 sec\n",
      "\n",
      "Epoch 15 Batch 0 Loss 0.0724\n",
      "Epoch 15 Batch 100 Loss 0.1279\n",
      "Epoch 15 Loss 0.0794\n",
      "Time taken for 1 epoch 306.0308005809784 sec\n",
      "\n",
      "Epoch 16 Batch 0 Loss 0.0459\n",
      "Epoch 16 Batch 100 Loss 0.0738\n",
      "Epoch 16 Loss 0.0721\n",
      "Time taken for 1 epoch 313.26618242263794 sec\n",
      "\n",
      "Epoch 17 Batch 0 Loss 0.0718\n",
      "Epoch 17 Batch 100 Loss 0.0765\n",
      "Epoch 17 Loss 0.0697\n",
      "Time taken for 1 epoch 306.01705718040466 sec\n",
      "\n",
      "Epoch 18 Batch 0 Loss 0.0664\n",
      "Epoch 18 Batch 100 Loss 0.0404\n",
      "Epoch 18 Loss 0.0676\n",
      "Time taken for 1 epoch 315.5625710487366 sec\n",
      "\n",
      "Epoch 19 Batch 0 Loss 0.0357\n",
      "Epoch 19 Batch 100 Loss 0.0928\n",
      "Epoch 19 Loss 0.0684\n",
      "Time taken for 1 epoch 307.5467178821564 sec\n",
      "\n",
      "Epoch 20 Batch 0 Loss 0.0507\n",
      "Epoch 20 Batch 100 Loss 0.0776\n",
      "Epoch 20 Loss 0.0837\n",
      "Time taken for 1 epoch 320.7970585823059 sec\n",
      "\n",
      "Epoch 21 Batch 0 Loss 0.0941\n",
      "Epoch 21 Batch 100 Loss 0.0941\n",
      "Epoch 21 Loss 0.0795\n",
      "Time taken for 1 epoch 307.74695014953613 sec\n",
      "\n",
      "Epoch 22 Batch 0 Loss 0.0561\n",
      "Epoch 22 Batch 100 Loss 0.0622\n",
      "Epoch 22 Loss 0.0779\n",
      "Time taken for 1 epoch 332.9381947517395 sec\n",
      "\n",
      "Epoch 23 Batch 0 Loss 0.0468\n",
      "Epoch 23 Batch 100 Loss 0.0992\n",
      "Epoch 23 Loss 0.0701\n",
      "Time taken for 1 epoch 249.29298186302185 sec\n",
      "\n",
      "Epoch 24 Batch 0 Loss 0.0274\n",
      "Epoch 24 Batch 100 Loss 0.0993\n",
      "Epoch 24 Loss 0.0686\n",
      "Time taken for 1 epoch 264.7950005531311 sec\n",
      "\n",
      "Epoch 25 Batch 0 Loss 0.0534\n",
      "Epoch 25 Batch 100 Loss 0.0679\n",
      "Epoch 25 Loss 0.0634\n",
      "Time taken for 1 epoch 250.9159426689148 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "EPOCHS = 25\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(EPOCHS):\n",
    "  start = time.time()\n",
    "\n",
    "  # Initialize the hidden state\n",
    "  enc_hidden = encoder.initialize_hidden_state()\n",
    "  total_loss = 0\n",
    "\n",
    "  # Loop through the dataset\n",
    "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "\n",
    "    # Call the train method\n",
    "    batch_loss = train_step(inp, targ, enc_hidden)\n",
    "\n",
    "    # Compute the loss (per batch)\n",
    "    total_loss += batch_loss\n",
    "\n",
    "    if batch % 100 == 0:\n",
    "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                   batch,\n",
    "                                                   batch_loss.numpy()))\n",
    "  # Save (checkpoint) the model every 2 epochs\n",
    "  if (epoch + 1) % 2 == 0:\n",
    "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "  # Output the loss observed until that epoch\n",
    "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / steps_per_epoch))\n",
    "  \n",
    "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6fe9380c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Evaluate function -- similar to the training loop\n",
    "def evaluate(sentence):\n",
    "\n",
    "  # Attention plot (to be plotted later on) -- initialized with max_lengths of both target and input\n",
    "  attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "\n",
    "  # Preprocess the sentence given\n",
    "  sentence = preprocess_sentence(sentence)\n",
    "\n",
    "  # Fetch the indices concerning the words in the sentence and pad the sequence\n",
    "  inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
    "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                         maxlen=max_length_inp,\n",
    "                                                         padding='post')\n",
    "  # Convert the inputs to tensors\n",
    "  inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "  result = ''\n",
    "\n",
    "  hidden = [tf.zeros((1, units))]\n",
    "  enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "  dec_hidden = enc_hidden\n",
    "  dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
    "\n",
    "  # Loop until the max_length is reached for the target lang (ENGLISH)\n",
    "  for t in range(max_length_targ):\n",
    "    predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                         dec_hidden,\n",
    "                                                         enc_out)\n",
    "\n",
    "    # Store the attention weights to plot later on\n",
    "    attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "    attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "    # Get the prediction with the maximum attention\n",
    "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "    # Append the token to the result\n",
    "    result += targ_lang.index_word[predicted_id] + ' '\n",
    "\n",
    "    # If <end> token is reached, return the result, input, and attention plot\n",
    "    if targ_lang.index_word[predicted_id] == '<end>':\n",
    "      return result, sentence, attention_plot\n",
    "\n",
    "    # The predicted ID is fed back into the model\n",
    "    dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "  return result, sentence, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "15e43927",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "# Function for plotting the attention weights\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "  fig = plt.figure(figsize=(10,10))\n",
    "  ax = fig.add_subplot(1, 1, 1)\n",
    "  ax.matshow(attention, cmap='viridis')\n",
    "\n",
    "  fontdict = {'fontsize': 14}\n",
    "\n",
    "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0490ed5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate function (which internally calls the evaluate function)\n",
    "def translate(sentence):\n",
    "  result, sentence, attention_plot = evaluate(sentence)\n",
    "\n",
    "  print('Input: %s' % (sentence))\n",
    "  print('Predicted translation: {}'.format(result))\n",
    "\n",
    "  attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
    "  plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "46848423",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x1c8d4d73550>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Restore the latest checkpoint in checkpoint_dir\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9a6654fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> je te prie de partir <end>\n",
      "Predicted translation: please leave . <end> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nabeel_Ahmed\\AppData\\Local\\Temp\\ipykernel_8376\\124968884.py:12: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
      "C:\\Users\\Nabeel_Ahmed\\AppData\\Local\\Temp\\ipykernel_8376\\124968884.py:13: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "translate(u\"Je te prie de partir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5684d9c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3202299671.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[32], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    https://blog.paperspace.com/seq-to-seq-attention-mechanism-keras/\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "https://blog.paperspace.com/seq-to-seq-attention-mechanism-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5b306b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84efa334",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4e39e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
